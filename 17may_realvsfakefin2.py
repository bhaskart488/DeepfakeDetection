# -*- coding: utf-8 -*-
"""realvsfakefin2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9OAJCJvH50r97bwbeiYvgZr6ZhHRg8F
"""

# /content/drive/MyDrive/realvsfakeus

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

!pip install tensorflow

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt

# setting the image fixed size for training and intializing the batch size, channel and number of epochs
Image_Size= 256
Batch_Size = 32
Channels=3
Epochs=55

# determining number of pics and classes
imgdata = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/realvsfakeus",
    shuffle=True,
    image_size = (Image_Size,Image_Size),
    batch_size=Batch_Size

)

# above code creates
# This code creates a dataset of images using the Keras utility function image_dataset_from_directory.
# It reads images from a specified directory and organizes them into batches for training or validation.
# Here’s what each parameter means:
# This is the path to the directory containing your image data.
# The function will look for subdirectories within this path, where each subdirectory
# corresponds to a different class or label (e.g., “real” and “fake” faces).
# Images in each subdirectory will be treated as examples of that class.

class_names = imgdata.class_names
class_names  # 0 means fake and 1 means real

for image_batch, label_batch in imgdata.take(1):
    print(image_batch.shape)
    print(label_batch.numpy())

# The above code iterates over the first batch of data from the dataset.
# The take(1) method ensures that only one batch is retrieved.
# Each batch contains a set of images and their corresponding labels.
# The .numpy() method converts the labels from TensorFlow tensors to NumPy
# arrays for easier printing.
# The labels correspond to the class names (“fake” or “real”) associated with each image

# Displaying our first image
for image_batch, label_batch in imgdata.take(1):
    plt.imshow(image_batch[0].numpy().astype("uint8"))
    plt.title(class_names[label_batch[0]])

# function to create dataset

def splitting_dataset_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):

    ds_size=len(ds)

    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    train_size=int(train_split * ds_size)
    val_size= int(val_split * ds_size)

    train_ds= ds.take(train_size)

    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)

    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds=splitting_dataset_tf(imgdata)

print(len(train_ds),len(val_ds),len(test_ds))

#  Pipeline for Optimization for Training, Validation, and Testing Datasets
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

# Image Preprocessing for Resizing and Rescaling
resize_and_rescale = tf.keras.Sequential([
    layers.experimental.preprocessing.Resizing(Image_Size,Image_Size),
    layers.experimental.preprocessing.Rescaling(1.0/255)
])

# Data augmentation
data_aug = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2),

])

# Architecture of the model
input_shape = (Batch_Size,Image_Size, Image_Size,Channels)
n_classes = 3

model = models.Sequential([
    resize_and_rescale,
    data_aug,
    layers.Conv2D(32, (3,3), activation='relu', input_shape = input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),


    layers.Flatten(),
    layers.Dense(64, activation = 'relu'),
    layers.Dense(n_classes, activation= 'softmax'),

])

model.build(input_shape=input_shape)

# Compiling the model with loss function and optimizer
model.compile(
    optimizer='adam',
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

# Model Training with Training and Validation Data
history = model.fit(
    train_ds,
    epochs=275,
    batch_size=Batch_Size,
    verbose=1,
    validation_data=val_ds
)

# checking the accuracy of the model
scores = model.evaluate(test_ds)

# accuracy of the model is 92.65%

# predicting labels for the batch of images
import numpy as np

for image_batch, label_batch in imgdata.take(1):

    first_image = image_batch[0].numpy().astype('uint8')
    first_label = label_batch[0].numpy()

    print("first image to predict")
    plt.imshow(first_image)
    print("Actual label : ",class_names[first_label])


    batch_pred = model.predict(image_batch)
    print("Pred label : ",class_names[np.argmax(batch_pred[0])])

# image prediction function using the model
def pred(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

# Displaying Sample Predictions with Confidence
plt.figure(figsize=(15, 15))

for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3,3, i+1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = pred(model, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual : {actual_class},\n Predicted:{predicted_class}.\n Confidence:{confidence}%")

        plt.axis("off")

# saving the model
import pickle

with open('model_f_real_pickle_final','wb') as f:
  pickle.dump(model,f)

# to run the pickle(saved model)
# import pickle

with open('model_f_real_pickle_final','rb') as f:
  model_saved = pickle.load(f)

#to predict the model
#model_saved.predict("give input")

plt.figure(figsize=(15, 15))

for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3,3, i+1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = pred(model_saved, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual : {actual_class},\n Predicted:{predicted_class}.\n Confidence:{confidence}%")

        plt.axis("off")

